<span style="font-size:50px">3ì£¼ì°¨ GMM</span>



- [Class 1](#class-1)
    - [ğŸ“Œ What is Machine Learning?](#-what-is-machine-learning)
      - [1. CS vs ML](#1-cs-vs-ml)
      - [2. Components of Machine Learning](#2-components-of-machine-learning)
    - [ğŸ“Œ Perceptron](#-perceptron)
      - [1. Provable Convergence Properties ( ì¦ëª… ê°€ëŠ¥í•œ ìˆ˜ë ´ ì„±ì§ˆ )](#1-provable-convergence-properties--ì¦ëª…-ê°€ëŠ¥í•œ-ìˆ˜ë ´-ì„±ì§ˆ-)
      - [2. How This Improved?](#2-how-this-improved)
    - [ğŸ“Œ ML vs AI](#-ml-vs-ai)
      - [1. Compare of ML, AI](#1-compare-of-ml-ai)
      - [2.Examples](#2examples)
    - [ğŸ“Œ Methods of Learning](#-methods-of-learning)
      - [1. Compare of Methods](#1-compare-of-methods)
      - [2. How Supervised Learning Operate?](#2-how-supervised-learning-operate)
          - [Setup](#setup)
          - [Examples of $\\mathcal{Y}$](#examples-of-mathcaly)
          - [Examples of $\\mathcal{X}$](#examples-of-mathcalx)
- [Class 2](#class-2)
    - [ğŸ“Œ Hypothesis Class](#-hypothesis-class)
      - [1. What is Hypothesis Class?](#1-what-is-hypothesis-class)
      - [2. Examples of Hypothesis Class](#2-examples-of-hypothesis-class)
          - [2-1. ì„ í˜• ì´ì§„ ë¶„ë¥˜ê¸°](#2-1-ì„ í˜•-ì´ì§„-ë¶„ë¥˜ê¸°)
          - [2-2. ë‹¤í•­ íšŒê·€](#2-2-ë‹¤í•­-íšŒê·€)
          - [2-3. CNN](#2-3-cnn)
      - [3. VC ì°¨ì›ì„ ì´ìš©í•œ ë³µì¡ë„ ì¸¡ì •](#3-vc-ì°¨ì›ì„-ì´ìš©í•œ-ë³µì¡ë„-ì¸¡ì •)
          - [3-1. VC ì°¨ì›ì´ë€?](#3-1-vc-ì°¨ì›ì´ë€)
          - [3-2. Generalization Bound](#3-2-generalization-bound)
    - [ğŸ“Œ Loss Function](#-loss-function)
      - [1. Defintion of Loss Function](#1-defintion-of-loss-function)
      - [2. Examples of Loss Function](#2-examples-of-loss-function)
          - [2-1. 1/0 loss](#2-1-10-loss)
          - [2-2. Square Loss](#2-2-square-loss)
          - [2-3. Absolute Loss](#2-3-absolute-loss)
      - [3. Train / Test Data Split](#3-train--test-data-split)
    - [ğŸ“Œ Example Of Doing Supervised Learning](#-example-of-doing-supervised-learning)
      - [1. Data Preprocess](#1-data-preprocess)
      - [2. Train / Test Split](#2-train--test-split)
      - [3. Justify Model](#3-justify-model)
      - [4. Justify Optimizer](#4-justify-optimizer)
      - [5. Learn](#5-learn)
      - [6. Evaluate Model](#6-evaluate-model)

# <span style="color:red">Class 1</span>

### ğŸ“Œ What is Machine Learning?

#### 1. CS vs ML
|ì •í†µ Computer Science|Machine Learning|
|---|---|
|Data + Program -> Computer -> Output|Data + Output -> Computer -> Program -> Computer -> Output|



> More data -> Better Program
>> The reason ML contains "Learning"

#### 2. Components of Machine Learning

1ï¸âƒ£ E (Expreince) <span style="color:skyblue">like data..</span>

2ï¸âƒ£ T (Task) <span style="color:skyblue">ëª©í‘œ</span>

3ï¸âƒ£ P (Perfomance) <span style="color:skyblue">ëª©í‘œë¥¼ ìœ„í•œ ì„±ëŠ¥ì§€í‘œ</span>

> <span style="color:red">"Algorithms that improve on some task with expreince"</span>

### ğŸ“Œ Perceptron

#### 1. Provable Convergence Properties ( ì¦ëª… ê°€ëŠ¥í•œ ìˆ˜ë ´ ì„±ì§ˆ )

> Perceptronì€ Provableí•œ ìˆ˜ë ´ ì„±ì§ˆì„ ê°€ì§„ë‹¤.
> ê·¸ëŸ¬ë©´ ì´ê²Œ ë¬´ìŠ¨ ëœ»ì¼ê¹Œ?

âœ… ì¶”ê°€ íƒêµ¬

Perceptronì€ ë°ì´í„°ê°€ ì„ í˜• ë¶„ë¦¬ ê°€ëŠ¥ (linearly sparable) í•  ë•Œë§Œ ìˆ˜ë ´ì´ ì¦ëª…ëœë‹¤.

âœ… ì„ í˜• ë¶„ë¦¬ ê°€ëŠ¥í•˜ë‹¤ëŠ” ê²ƒì€?

â—ï¸ì§ê´€ì  ì •ì˜ : í•˜ë‚˜ì˜ ì§ì„  (í˜¹ì€ ì´ˆí‰ë©´)ìœ¼ë¡œ ë‘ í´ë˜ìŠ¤ë¥¼ ì™„ë²½íˆ ë‚˜ëˆŒ ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ëœ»í•œë‹¤.

> ex) AND ì—°ì‚°ì´ ì—¬ê¸°ì— í•´ë‹¹í•œë‹¤.

![alt text](image.png)
ì¶œì²˜ : [[ì¸ê³µì§€ëŠ¥] ì¸ê³µì‹ ê²½ë§(ANN:Artificial Nerual Network) Part 2](https://destiny738.tistory.com/455)

â—ï¸ìˆ˜í•™ì  ì •ì˜

ë°ì´í„°
$$
D = \{(x_1, y_1), (x_2, y_2), ... , (x_N, y_N)\}
$$

ì´ ì£¼ì–´ì¡Œì„ ë•Œ, ê° $x_i \in \mathbb{R}^n, y_i \in \{  -1, +1\}$ ë¼ê³  í•˜ì.
</br>
</br>
</br>
ì´ ë°ì´í„° ì…‹ì´ ì„ í˜• ë¶„ë¦¬ ê°€ëŠ¥í•˜ë‹¤ëŠ” ê²ƒì€,

ì–´ë–¤ ê°€ì¤‘ì¹˜ ë²¡í„° $\vec{w}\in\mathbb{R}^n$, ê·¸ë¦¬ê³  $ b \in \mathbb{R}$ê°€ ì¡´ì¬í•´ì„œ ì•„ë˜ ì¡°ê±´ì„ ëª¨ë“  ë°ì´í„°ì— ëŒ€í•´ ë§Œì¡±í•˜ëŠ” ê²½ìš°ì— í•´ë‹¹í•œë‹¤.

$$y_i (\vec{w}^{\top} \vec{x_i} + b) > 0, \forall i = 1,...,N$$

ê°„ë‹¨í•˜ê²Œ 2ì°¨ì›ì˜ ê²½ìš°ì—, ëª¨ë¸ì„ $ y = wx + b$ë¡œ ì •ì˜í•  ìˆ˜ ìˆëŠ”ë°, ì´ ê²½ìš°ì—ëŠ” $x = -\frac{b}{w}$ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í´ë˜ìŠ¤ê°€ +1ì¸ ë°ì´í„°ê°€ ëª¨ë‘ ì™¼ìª½ì— ìˆê±°ë‚˜ ì˜¤ë¥¸ìª½ì— ìˆì„ ë•Œ, ì„ í˜• ë¶„ë¦¬ ê°€ëŠ¥í•˜ë‹¤ê³  í•œë‹¤.

ë˜í•œ, $-\frac{b}{w}$ë¥¼ ê²°ì • ê²½ê³„ (Decision Boundary)
ë¼ê³ í•œë‹¤.

ğŸ’­ ì°¸ê³  1 : $\vec{w}^{\top}$ (ì „ì¹˜ í–‰ë ¬)ì„ ë§Œë“œëŠ” ì´ìœ ëŠ” í–‰ë ¬ê³±ì„ ìœ„í•´ì„œì„.

ğŸ’­ ì°¸ê³  2 : í¼ì…‰íŠ¸ë¡ ì€ ì´ì§„ ë¶„ë¥˜ë§Œ ì‹œí–‰í•˜ê¸° ë•Œë¬¸ì— -1, +1ë¡œ í´ë˜ìŠ¤ë¥¼ ë‘˜ ìˆ˜ ìˆëŠ” ê²ƒì„.

ğŸ’­ ì°¸ê³  3 : $\vec{w}^{\top} \vec{x_i}$ë¥¼ ì´ˆí‰ë©´ì´ë¼ê³  í•˜ë©°, 2ì°¨ì›ì˜ ê²½ìš°ì—ëŠ” ì§ì„ , 3ì°¨ì›ì˜ ê²½ìš°ì—ëŠ” í‰ë©´ì´ ëœë‹¤.

ğŸ’­ ex) 2ì°¨ì›ì˜ ì´ˆí‰ë©´ì€ $ax + by + c = 0$

ğŸ’­ ì¼ë°˜í™”í•˜ë©´, nì°¨ì›ì˜ ì´ˆí‰ë©´ì€ $a_1x_1 + a_2x_2 + ... + a_nx_n$

ì¦‰, ì´ ê²½ìš°ì—ëŠ” í•™ìŠµì´ ë¬´í•œíˆ ë°˜ë³µë˜ì§€ ì•Šìœ¼ë©° ì •í•´ì§„ íšŸìˆ˜ì•ˆì— ìˆ˜ë ´í•˜ê²Œ ëœë‹¤.


#### 2. How This Improved?

> ANN (Artifcial Nerual Network)

> Deep Learning


### ğŸ“Œ ML vs AI

#### 1. Compare of ML, AI

|ML|AI|
|---|---|
|Bottum-Up|Top-Down|
|ë°ì´í„°-íŒ¨í„´-ê²°ë¡ |ëª©í‘œì„¤ì •-í•´ê²°ë°©ë²•ì°¾ê¸°|
|í†µê³„ + ìµœì í™” ì¤‘ì‹¬|like Rule Based Machine|
|uncertantiyë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆìŒ.|uncertantiyë¥¼ ì²˜ë¦¬í•  ìˆ˜ ì—†ìŒ.|

#### 2.Examples

2-1. game-bot

2-2. spam filter

2-3. web search

### ğŸ“Œ Methods of Learning

#### 1. Compare of Methods

|Supervised Learning|Unsupervised Learning|Reinforcement Learning|
|---|---|---|
|ì§€ë„í•™ìŠµ|ë¹„ì§€ë„í•™ìŠµ|ê°•í™”í•™ìŠµ|
|labled examples|find patterns|delayed feedback|

#### 2. How Supervised Learning Operate?

###### Setup

$$Data = \{(\vec{x_1}, y_1), ... ,(\vec{x_n}, y_n)\} \le\mathcal{X}, \mathcal{Y}$$

$$(\mathcal{X} \sim \mathbb{R^d}, \mathcal{Y}ëŠ” ì˜ˆì¸¡ëª©í‘œ)$$

$$(\vec{x_n}, y_n)\sim \mathcal{P}$$

###### Examples of $\mathcal{Y}$

ì´ì§„ ë¶„ë¥˜
$$\mathcal{Y} = \{0, 1\}$$
$$\mathcal{Y} = \{-1, +1\}$$

ë‹¤ì¤‘ ë¶„ë¥˜
$$\mathcal{Y} = \{1, ..., k\}$$

íšŒê·€ ëª¨ë¸ 
$$\mathcal{Y} = \mathbb{R}$$


###### Examples of $\mathcal{X}$

ì´ì§„ ë¶„ë¥˜

$$\vec{x_n} = \begin{bmatrix} 1 \\ 62 \\ 182 \\ . \\.\\.\end{bmatrix}$$
(íŠ¹ì§• ë²¡í„°)


# <span style="color:red">Class 2</span>

### ğŸ“Œ Hypothesis Class

#### 1. What is Hypothesis Class?

ê·¸ë‹ˆê¹, ì–´ë–¤ ì¸ê³µì§€ëŠ¥ ëª¨ë¸ì„ ë§Œë“¤ê¸° ìœ„í•´ ê°€ëŠ¥í•œ í•¨ìˆ˜ë“¤ì˜ ì§‘í•©ì„.

ì´ê±¸ ì˜ ì„¤ì •í•˜ëŠ” ê²ƒì´ ì¸ê³µì§€ëŠ¥ ëª¨ë¸ì˜ <span style="font-size:50px">í•µì‹¬</span>ì„.

í•œê¸€ë¡œëŠ”, ê°€ì„¤ ì§‘í•©ì´ë¼ê³ ë„ í•¨.

ì˜ˆë¥¼ ë“¤ì–´, ì–´ë–¤ ë°ì´í„°ë¥¼ ë‹¨ìˆœíˆ ì„ í˜• ë¶„ë¥˜ë§Œ ì§„í–‰í•  ìˆ˜ ìˆì§€ë§Œ, ë‹¤í•­ ë¶„ë¥˜ë¥¼ ì‚¬ìš©í•˜ë©´ ëª¨ë¸ì˜ í•¨ìˆ˜ê°€ ë‹¬ë¼ì§€ê²Œ ë¨.

> ì„ í˜• ë¶„ë¥˜ì˜ ê²½ìš° $h(x) = ax+b$

> ë‹¤í•­ ë¶„ë¥˜ì˜ ê²½ìš° $h(x) = w_nx^n + ... + w_1x + b$ 

<span style="font-size:25px">ì¼ë°˜ì ìœ¼ë¡œ </span>ì„ í˜• ë¶„ë¥˜ ëª¨ë¸ì„ ì‚¬ìš©í•˜ë©´ êµ¬í•´ì•¼í•  ê°€ì¤‘ì¹˜ì˜ ìˆ˜ê°€ ì¤„ì–´ë“¤ì–´ ë¹ ë¥¸ í•™ìŠµì´ ê°€ëŠ¥í•˜ì§€ë§Œ, ê·¸ë§Œí¼ ì •í™•ë„ëŠ” ë–¨ì–´ì§€ë©°, ë‹¤í•­ ë¶„ë¥˜ë¥¼ ì‚¬ìš©í•˜ë©´ ì •í™•ë„ëŠ” ë†’ì•„ì§€ì§€ë§Œ, êµ¬í•´ì•¼í•  ê°€ì¤‘ì¹˜ì˜ ìˆ˜ê°€ ë§ì•„ì ¸ì„œ í•™ìŠµì´ ì˜¤ë˜ê±¸ë¦¼.

(ë°˜ë©´, ê³¼ì í•©ì˜ ê²½ìš°ë„ ìˆëŠ”ë°, ì´ëŠ” ì˜¤íˆë ¤ ë„ˆë¬´ ì¢‹ì€ ëª¨ë¸ì„ ì‚¬ìš©í•´ì„œ ì •í™•í•œ ê°’ì„ ì˜ˆì¸¡í•˜ì§€ ëª»í•˜ëŠ” ê²½ìš°ì„.)

#### 2. Examples of Hypothesis Class

###### 2-1. ì„ í˜• ì´ì§„ ë¶„ë¥˜ê¸°

ë¬¸ì œ
- ì…ë ¥ : $\vec{x} \in \mathbb{R^n}$
- ì¶œë ¥ : $y \in \{-1, +1\}$

Hypotheis Class
$$\mathcal{H} = \{h_{\vec{w}, b}(\vec{x}) = sign (\vec{w}\vec{x}+b)|\vec{w} \in \mathbb{R^n}, b \in \mathbb{R}\}$$

- ëª¨ë“  ê°€ëŠ¥í•œ ì„ í˜• ë¶„ë¥˜ê¸°ë¥¼ ë‚˜íƒ€ëƒ„.
- $\vec{w}$ëŠ” ê°€ì¤‘ì¹˜ ë²¡í„°, bëŠ” í¸í–¥
- sign í•¨ìˆ˜ëŠ” ì–‘ìˆ˜ëŠ” 1, ìŒìˆ˜ë©´ -1ì„ ì¶œë ¥
  
###### 2-2. ë‹¤í•­ íšŒê·€

ë¬¸ì œ
- ì…ë ¥ : $\vec{x} \in \mathbb{R^n}$
- ì¶œë ¥ : $\vec{y} \in \mathbb{R^n}$

Hypotheis Class (2ì°¨ ë‹¤í•­ì‹)
$$\mathcal{H} = \{h(\vec{x}) = a_2x^2 + a_1x +a_0|a_0, a_1,a_2 \in \mathbb{R}\}$$

- 2ì°¨ í•¨ìˆ˜ë¡œ ì´ë£¨ì–´ì§„ í´ë˜ìŠ¤
- ëª¨ë“  ì°¨ìˆ˜ì˜ ë‹¤í•­ì‹ì„ ë‹¤ í—ˆìš©í•˜ë©´,

$$\mathcal{H_{\infin}}= \{h(x) = \sum_{k=0}^{\infin}|a_k \in \mathbb{R}\}$$
ë‹¤ë§Œ, ì´ í•¨ìˆ˜ëŠ” ê³¼ì í•©ì˜ ê°€ëŠ¥ì„±ì´ ë†’ìŒ.

###### 2-3. CNN

CNNì„ ê¸°ì¤€ìœ¼ë¡œ ì„¤ëª…í•˜ë©´, ì•„ë˜ì™€ ê°™ì€ ì„¤ì •ì„ ë³€ê²½í•´ì„œ í•¨ìˆ˜ë¥¼ ì„¤ì •í•  ìˆ˜ ìˆë‹¤.
- ì¸µì˜ ìˆ˜
- ì»¤ë„ í¬ê¸°
- í’€ë§ ì—¬ë¶€ ë° ì¢…ë¥˜
- íŒ¨ë”© ë°©ì‹
- ë¹„ì„ í˜• í™œì„±í™” í•¨ìˆ˜
- í•™ìŠµë£°, ì˜µí‹°ë§ˆì´ì € ë“± í•˜ì´í¼íŒŒë¼ë¯¸í„°

ì´ê±¸ ì˜ ì„¤ì •í•˜ë©´ ì¢‹ì€ ëª¨ë¸ì„ ë§Œë“¤ ìˆ˜ ìˆë‹¤.

#### 3. VC ì°¨ì›ì„ ì´ìš©í•œ ë³µì¡ë„ ì¸¡ì •

âœ… ì¶”ê°€ íƒêµ¬

ë¨¸ì‹ ëŸ¬ë‹ì—ì„œ ëª¨ë¸ì´ ì–¼ë§ˆë‚˜ ì í•©í•œì§€, ê³¼ì í•© ìœ„í—˜ì€ ì–¼ë§ˆë‚˜ ë˜ëŠ”ì§€ë¥¼ ìˆ˜í•™ì ìœ¼ë¡œ í‰ê°€í•˜ëŠ” ë°©ë²•ì„.

###### 3-1. VC ì°¨ì›ì´ë€?

VCì°¨ì› (Vanpik-Chervonekis Dimension)ì€
> ì–´ë–¤ hypothesis class $\mathcal{H}$ê°€ ì™„ë²½í•˜ê²Œ êµ¬ë¶„(shatter)í•  ìˆ˜ ìˆëŠ” ë°ì´í„° í¬ì¸íŠ¸ì˜ ê°œìˆ˜

ë¼ê³ í•œë‹¤. ì¦‰, ëª¨ë¸ì´ ì–¼ë§ˆë‚˜ ë‹¤ì–‘í•œ ë¶„ë¥˜ë¥¼ ë‚¼ ìˆ˜ ìˆëŠëƒë¥¼ ìˆ˜í•™ì ìœ¼ë¡œ ì¸¡ì •í•œ ê°’ì´ë‹¤.

ì˜ˆë¥¼ ë“¤ì–´, 1ì°¨ ì„ í˜• ë¶„ë¥˜ê¸°ì˜ ì˜ˆì‹œë¥¼ ì‚´í´ë³´ì.

![alt text](image-1.png)

ì¶œì²˜ : [5. ëª¨ë¸ê³¼ ëª¨ë¸ë³µì¡ë„(Vapnik-Chervonenkis Dimension, VC Dimension)|ì‘ì„±ì wpxkxmfpdls](https://blog.naver.com/wpxkxmfpdls/221688936973)

ì´ ê²½ìš°, ë°ì´í„°ì˜ ê°œìˆ˜ê°€ 2ì¼ë•Œ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë‘ë¥¼ ë¶„ë¥˜í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.

í•˜ì§€ë§Œ, ë°ì´í„° ê°œìˆ˜ê°€ 3ê°œì¸ ê²½ìš°ë¥¼ ì‚´í´ë³´ì

![alt text](image-2.png)

ë‹¤ìŒê³¼ ê°™ì€ 8ê°€ì§€ ê²½ìš°ê°€ ìˆì„ ìˆ˜ ìˆëŠ”ë°, ì´ë“¤ ì¤‘ ëª‡ê°€ì§€ëŠ” ëª¨ë‘ë¥¼ ë¶„ë¥˜í•  ìˆ˜ ìˆëŠ” ëª¨ë¸ì„ êµ¬í•  ìˆ˜ ì—†ë‹¤. ë”°ë¼ì„œ, ì´ ê²½ìš° VC ì°¨ì›ì€ 2ì´ë‹¤.

í•˜ì§€ë§Œ, ëª¨ë“  ëª¨ë¸ì— ëŒ€í•´ì„œ ì´ë¥¼ ì§ì ‘ ì‹¤í–‰í•˜ëŠ” ê²ƒì€ ì–´ë µë‹¤.

ë”°ë¼ì„œ, upper boundë¥¼ ì‚¬ìš©í•´ VC ì°¨ì›ì„ ê²°ì •í•œë‹¤.

- ë‹¤í•­ì‹ í•¨ìˆ˜ of degree d: VC ì°¨ì› = d+1
- dì°¨ì› ê³µê°„ì—ì„œì˜ ì„ í˜• ë¶„ë¥˜ê¸° (hyperplane): VC = d+1
- ì‹ ê²½ë§ (íŒŒë¼ë¯¸í„° ìˆ˜ W)
  $$VC Dimension \le c * W * log W$$
  ì—¬ê¸°ì„œ cëŠ” ìƒìˆ˜

###### 3-2. Generalization Bound

$$
\mathbb{P} \left[ \sup_{h \in \mathcal{H}} \left| \text{trueError}(h) - \text{empiricalError}(h) \right| > \epsilon \right] \leq \delta
$$

$$
forall \epsilon > 0, \forall \delta \in (0, 1), \text{ with probability at least } 1 - \delta:
\quad
\sup_{h \in \mathcal{H}} \left| \text{trueError}(h) - \text{empiricalError}(h) \right|
\leq
\sqrt{ \frac{d \left( \log\left( \frac{2n}{d} \right) + 1 \right) + \log\left( \frac{4}{\delta} \right) }{n} }
$$

ğŸ’­ ì•„ì§ ì´í•´í•˜ì§€ ëª»í–ˆë‹¤.

### ğŸ“Œ Loss Function


#### 1. Defintion of Loss Function


Loss Function (ì†ì‹¤ í•¨ìˆ˜)ë€, ëª¨ë¸ì´ ì˜ˆì¸¡í•œ ê°’ê³¼ ì‹¤ì œ ê°’ ì‚¬ì´ì˜ ì°¨ì´ë¥¼ ì¸¡ì •í•˜ëŠ” í•¨ìˆ˜ì´ë‹¤.

#### 2. Examples of Loss Function

###### 2-1. 1/0 loss

ì •ë‹µì´ ë§ìœ¼ë©´ 0, ë§ì§€ ì•Šìœ¼ë©´ 1

###### 2-2. Square Loss

$$
l(h, D) = \frac{1}{n} \sum_{i}^{n} (h(x_i - y_i))^2
$$

###### 2-3. Absolute Loss

$$
l(h, D) = \frac{1}{n} \sum_{i}^{n} |h(x_i - y_i)|
$$

#### 3. Train / Test Data Split

Loss Functionì„ ê³„ì‚°í• ë•Œ, Train Datasetê³¼ Test Datasetì´ ê°™ìœ¼ë©´ Lossê°€ ë°œìƒí•  ìˆ˜ ì—†ë‹¤. ë”°ë¼ì„œ, ì´ë¥¼ ìœ„í•´ Splitì´ í•„ìš”í•˜ë‹¤.

### ğŸ“Œ Example Of Doing Supervised Learning

#### 1. Data Preprocess
#### 2. Train / Test Split
#### 3. Justify Model
#### 4. Justify Optimizer
#### 5. Learn
#### 6. Evaluate Model