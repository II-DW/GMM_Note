
- [Book Chapter 4.](#book-chapter-4)
    - [ğŸ“Œ What is Loss Function](#-what-is-loss-function)
      - [1. Definition of Loss Function](#1-definition-of-loss-function)
    - [ğŸ“Œ What is Optimizer](#-what-is-optimizer)
      - [2. Optimizer](#2-optimizer)
        - [2-1. ê¸°ë³¸í˜•: Gradient Descent (GD)](#2-1-ê¸°ë³¸í˜•-gradient-descent-gd)
          - [2-1-1. Batch Gradient Descent](#2-1-1-batch-gradient-descent)
          - [2-1-2. Stochastic Gradient Descent (SGD)](#2-1-2-stochastic-gradient-descent-sgd)
          - [2-1-3. BGD vs SGD](#2-1-3-bgd-vs-sgd)
        - [2-2. ëª¨ë©˜í…€ ê³„ì—´ optimizer](#2-2-ëª¨ë©˜í…€-ê³„ì—´-optimizer)
          - [2-2-1. Momentum](#2-2-1-momentum)
          - [2-2-2. Nesterov Accelerated Gradient](#2-2-2-nesterov-accelerated-gradient)
        - [2-3. ì ì‘í˜• í•™ìŠµë¥  optimizer](#2-3-ì ì‘í˜•-í•™ìŠµë¥ -optimizer)
          - [2-3-1. Adagrad](#2-3-1-adagrad)
          - [2-3-2. RMSprop](#2-3-2-rmsprop)
          - [2-3-3. Adadelta](#2-3-3-adadelta)
        - [2-4. Adam ê³„ì—´](#2-4-adam-ê³„ì—´)
          - [2-4-1. Adam (Adaptive Moment Estimation)](#2-4-1-adam-adaptive-moment-estimation)
          - [2-4-2. Adamax](#2-4-2-adamax)
          - [2-4-3. Nadam](#2-4-3-nadam)
        - [2-5. ìµœê·¼ ê³ ê¸‰ Optimizer](#2-5-ìµœê·¼-ê³ ê¸‰-optimizer)
- [Reference](#reference)


# <span style="color:red">Book Chapter 4.</span>

### ğŸ“Œ What is Loss Function

#### 1. Definition of Loss Function

Loss Functionì€ ìš°ë¦¬ê°€ ë§Œë“  ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’ì´ ì‹¤ì œ ë°ì´í„°ì™€ ì°¨ì´ê°€ ì–¼ë§ˆë‚˜ ë‚˜ëŠ”ì§€ ìˆ˜ì¹˜ì ìœ¼ë¡œ ì •ëŸ‰í™”í•œ ì§€í‘œì´ë‹¤.

í•™ìŠµ, ì¦‰ fittingì„ ì§„í–‰í•  ë•Œ, ìš°ë¦¬ëŠ” input data xì™€ output data y ì‚¬ì´ì˜ ê´€ê³„ (mapping)ì„ ì°¾ì•„ê°€ê²Œ ëœë‹¤. ì´ ê³¼ì •ì—ì„œ ê°€ì¤‘ì¹˜ë¥¼ ì¡°ì •í•˜ê²Œ ë˜ë©°, loss functionì´ ì ì ˆí•œ ê°€ì¤‘ì¹˜ë¥¼ ì°¾ì•„ê°€ëŠ” ë°©í–¥ì„ ì œì‹œí•˜ê²Œ ë˜ëŠ” ê²ƒì´ë‹¤.

### ğŸ“Œ What is Optimizer

#### 2. Optimizer

lossë¥¼ ì¤„ì´ëŠ” ë°©í–¥ìœ¼ë¡œ í•™ìŠµì„ ì§„í–‰ì‹œí‚¤ëŠ” ì•Œê³ ë¦¬ì¦˜ì„ optimizerë¼ê³  í•œë‹¤. ê·¸ëŸ¼ ì–´ë–¤ optimizerê°€ ìˆì„ê¹Œ?


##### 2-1. ê¸°ë³¸í˜•: Gradient Descent (GD)

###### 2-1-1. Batch Gradient Descent

- ì „ì²´ ë°ì´í„°ì…‹ì„ ê¸°ì¤€ìœ¼ë¡œ í•œ ë²ˆì˜ ê²½ì‚¬ í•˜ê°•
- ì¥ì  : ìˆ˜ë ´ ì•ˆì •ì .
- ë‹¨ì  : ëŒ€ê·œëª¨ ë°ì´í„°ì—ì„  ëŠë¦¼, ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í¼.
  
```Matlab
% This code is written by Matlab
function W = DeltaBatch(W, X, D)
    alpha =0.9;
    dWsum = zeros(3, 1);
    N =4;
    for k =1:N
        x = X(k, :);
        d = D(k);
        
        v = W*x;
        y = Sigmoid(v);
        e = d - y;
        delta = y*(1-y)*e
        dW = alpha * delta * x;
        
        dWsum = dWsum + dW;
    end
    dWavg = dWsum / N;
    W(1) = W(1) + dWavg(1);
    W(2) = W(2) + dWavg(2);
    W(3) = W(3) + dWavg(3);
end
```

###### 2-1-2. Stochastic Gradient Descent (SGD)

- ë°ì´í„° í•˜ë‚˜(ìƒ˜í”Œ)ë§ˆë‹¤ ê°€ì¤‘ì¹˜ ê°±ì‹ .
- ì¥ì  : ë©”ëª¨ë¦¬ íš¨ìœ¨ì , ë¹ ë¥¸ ì—…ë°ì´íŠ¸.
- ë‹¨ì  : ì§„ë™ì´ ì‹¬í•˜ê³  ìµœì ì  ê·¼ì²˜ì—ì„œ ë¶ˆì•ˆì •.

```Matlab
% This code is written by Matlab
function W = DeltaSGD(W, X, D)
    alpha =0.9;
    N =4;
    for k =1:N
        x = X(k, :);
        d = D(k);
        v = W*x;
        y = Sigmoid(v);
        e = d - y;
        delta = y*(1-y)*e;
        dW = alpha*delta*x;
        W(1) = W(1) + dW(1);
        W(2) = W(2) + dW(2);
        W(3) = W(3) + dW(3);
    end
end
```

###### 2-1-3. BGD vs SGD

ì•„ë˜ì™€ ê°™ì€ ì½”ë“œë¥¼ í†µí•´ ë‘ ëª¨ë¸ì„ í™œìš©í•´ í•™ìŠµì„ ì§„í–‰í•˜ê³ , ì´ë“¤ ì‚¬ì´ì˜ ì°¨ì´ì ì„ ì°¾ì•„ë³´ì. 
```Matlab
% This code is written by Matlab
X = [0 0 1;
      0 1 1;
      1 0 1;
      1 1 1;
    ];
D = [0
    0
    1
    1
    ];
E1 = zeros(1000,1); 
E2 = zeros(1000,1);
W1 =2*rand(1, 3) -1;
W2 = W1;
for epoch =1:1000
    W1 = DeltaSGD(W1, X, D);
    W2 = DeltaBatch(W2, X, D);
    es1 =0;
    es2 =0;
    N =4;
    for k =1:N
        x = X(k, :);
        d = D(k);
        
        disp(size(x));
        disp(size(W1));
        v1 = W1*x;
        y1 = Sigmoid(v1);
        es1 = es1 + (d-y1)^2;
        v2 = W2*x;
        y2 = Sigmoid(v2);
        es2 = es2 + (d-y2)^2;
    end
    E1(epoch) = es1 / N;
    E2(epoch) = es2 / N;
end
plot(E1, "r")
hold on
plot(E2, 'b:')
xlabel('Epoch')
ylabel('Average f Trainig Error')
legend('SGD', 'Batch')
```

ê²°ê³¼ì ìœ¼ë¡œ ë‚˜ì˜¨ ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

![alt text](image.png)

##### 2-2. ëª¨ë©˜í…€ ê³„ì—´ optimizer

ë¬´ì—‡ì„ ìœ„í•œ ê²ƒì¸ê°€?

-> ì§„ë™ ì–µì œ, ìˆ˜ë ´ ê°€ì†ì„ ìœ„í•´ ê³¼ê±°ì˜ ë°©í–¥ì„±ì„ ê³ ë ¤í•œ ìµœì í™” ë„ì…


###### 2-2-1. Momentum

- ê³¼ê±° ê¸°ìš¸ê¸°ì˜ ì´ë™ í‰ê· ì„ ì´ìš©í•´ ë°©í–¥ ìœ ì§€, ì´ë¦„ì²˜ëŸ¼ "ê´€ì„±"ì„ ì£¼ëŠ” ë°©ì‹ì„. ì´ì „ì— ê°”ë˜ ë°©í–¥ì„ ê¸°ì–µí•´ì„œ, ê·¸ ë°©í–¥ìœ¼ë¡œ ë”ìš± ë°€ì–´ì£¼ëŠ” ëŠë‚Œ.
- ì¥ì  : ê²½ì‚¬ ì§„ë™ ì–µì œ, ìˆ˜ë ´ ì†ë„ ì¦ê°€ (ì´ëŠ” ê¸°ìš¸ê¸°ê°€ ì™”ë‹¤ê°”ë‹¤ í•˜ë©´ì„œë„ íë¦„ì„ ìœ ì§€í•˜ë©´ì„œ ë” ë¹¨ë¦¬ê°ˆ ìˆ˜ ìˆê¸° ë•Œë¬¸)
- ìˆ˜ì‹ : 
$$
\begin{align*}
v_t &= \gamma v_{t-1} + \eta \nabla J(\theta) \\
\theta &= \theta - v_t
\end{align*}
$$

###### 2-2-2. Nesterov Accelerated Gradient

- ê¸°ì¡´ì˜ Momentum ë°©ì‹ì€ ê¸°ìš¸ê¸°ë¥¼ ì—…ë°ì´íŠ¸í•˜ê³ , momentumë§Œí¼ ì í”„í•˜ëŠ” ë°˜ë©´, ì´ ë°©ì‹ì€ momentumë§Œí¼ ì í”„í•˜ê³ , ê¸°ìš¸ê¸°ë¥¼ ì—…ë°ì´íŠ¸í•œë‹¤.
- ì¥ì  : ë” ë˜‘ë˜‘í•˜ê²Œ ë°©í–¥ì„ ì¡ìŒ â†’ ë” ë¹ ë¥´ê³  ì •í™•í•œ ìˆ˜ë ´ ê°€ëŠ¥.

##### 2-3. ì ì‘í˜• í•™ìŠµë¥  optimizer

###### 2-3-1. Adagrad

- íŒŒë¼ë¯¸í„°ë§ˆë‹¤ ë‹¤ë¥¸ ì†ë„ë¡œ í•™ìŠµí•¨.
- ìì£¼ ë“±ì¥í•˜ëŠ” ë³€ìˆ˜ëŠ” ì—…ë°ì´íŠ¸ë¥¼ ì¤„ì´ê³ , ë“œë¬¸ ë³€ìˆ˜ëŠ” ìì£¼ ë°”ê¾¼ë‹¤.
- ì–´ë–»ê²Œ íŒŒë¼ë¯¸í„°ë§ˆë‹¤ ë“±ì¥í•˜ëŠ” íšŸìˆ˜ê°€ ë‹¤ë¥¼ ìˆ˜ ìˆëŠ”ê°€? => ì¦‰ ì–¼ë§ˆë‚˜ ì—…ë°ì´íŠ¸ë˜ëŠ”ì§€, ê·¸ íšŸìˆ˜ë¥¼ ê³„ì‚°í•´ë‘ê³  ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•¨.
- ë‹¨ì  : í•™ìŠµë¥ ì´ ì ì  ë„ˆë¬´ ì‘ì•„ì ¸ì„œ í•™ìŠµì´ ë©ˆì¶”ëŠ” ê²½ìš°ë„ ìˆìŒ.
  
###### 2-3-2. RMSprop

- Adagradê°€ ë°°ì›€ì˜ ì–‘ì„ ëˆ„ì í•˜ê¸° ë•Œë¬¸ì— ë‚˜ì¤‘ì— ë©ˆì¶”ëŠ” ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë“±ì¥
- RMSpropì€ ìµœê·¼ ê²ƒë“¤ë§Œ ì¤‘ìš”í•˜ê²Œ ë´„. (ê³¼ê±°ì˜ ê²ƒì€ ìŠê³  ì§€ë‚˜ê°.)
- ì¥ì  : í•™ìŠµì´ ë©ˆì¶”ì§€ì•Šê³  ì˜¤ë˜ ìœ ì§€ë¨. -> RNNì—ì„œ ìœ ë¦¬

###### 2-3-3. Adadelta

- RMSpropì„ ë°œì „ì‹œí‚¨ í˜•íƒœ, í•™ìŠµë¥  ìì²´ë„ ìë™ìœ¼ë¡œ ì¡°ì •í•¨.
- ì¥ì  : ì‚¬ëŒì´ ì§ì ‘ í•™ìŠµë¥ ì„ ì •í•  í•„ìš”ê°€ ì—†ìŒ.

##### 2-4. Adam ê³„ì—´

###### 2-4-1. Adam (Adaptive Moment Estimation)

- Momentumê³¼ RMSpropì„ í•©ì¹œ ê²ƒ!
- ë°©í–¥ì„±ë„ ê¸°ì–µí•˜ê³  ë³€ìˆ˜ë³„ë¡œ ì†ë„ë„ ë‹¤ë¥´ê²Œ ì„¤ì •í•¨.
- ì¥ì  : ê±°ì˜ ëª¨ë“  ìƒí™©ì—ì„œ ì¢‹ì€ ì„±ëŠ¥ì„ ê°€ì§€ë©°, ê·¸ë˜ì„œ ê¸°ë³¸ê°’ì²˜ëŸ¼ ìì£¼ ì‚¬ìš©ë¨.

###### 2-4-2. Adamax

- Adamì˜ ë²„ì „ ì¤‘ í•˜ë‚˜ì¸ë° íŠ¹ì´ê°’ì´ë‚˜ í° ê°’ì— ë” ê°•í•¨.
- L2ê°€ ì•„ë‹ˆë¼ L$\infin$(ê°€ì¥ í° ê°’ ê¸°ì¤€)ìœ¼ë¡œ ì¡°ì ˆí•¨.
- ì¥ì  : ì•ˆì •ì„± ì¦ê°€, í° ê°’ì— ëŒ€í•œ ë¯¼ê°ë„ ì¤„ì–´ë“¦

###### 2-4-3. Nadam

- Adam + NAG (ì˜ˆì¸¡ ê¸°ë°˜ ì—…ë°ì´íŠ¸)
- Adamì´ ì¢‹ê¸´ í•œë°, 'ì˜ˆì¸¡í•´ì„œ ì›€ì§ì´ëŠ”' ì˜ë¦¬í•¨ê¹Œì§€ ì¶”ê°€ëœ ëŠë‚Œ
- ì¥ì  : ë” ë¶€ë“œëŸ½ê³  ë¹ ë¥´ê²Œ ìˆ˜ë ´.

##### 2-5. ìµœê·¼ ê³ ê¸‰ Optimizer

- AMSGrad
- AdaBound (2019)
- Lookahead (2019)
- Lion (2023, Meta AI)

# Reference

[ë°‘ë°”ë‹¥ë¶€í„° ì‹œì‘í•˜ëŠ” ë”¥ëŸ¬ë‹ - ì‚¬ì´í†  ì½”í‚¤](https://product.kyobobook.co.kr/detail/S000215599933)

[ë”¥ëŸ¬ë‹ ì²«ê±¸ìŒ - ê¹€ì„±í•„](https://product.kyobobook.co.kr/detail/S000001057884)