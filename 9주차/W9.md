
- [Book Chapter 4.](#book-chapter-4)
    - [ğŸ“Œ What is Loss Function](#-what-is-loss-function)
      - [1. Definition of Loss Function](#1-definition-of-loss-function)
      - [2. Optimizer](#2-optimizer)
        - [2-1. ê¸°ë³¸í˜•: Gradient Descent (GD)](#2-1-ê¸°ë³¸í˜•-gradient-descent-gd)
          - [2-1-1. Batch Gradient Descent](#2-1-1-batch-gradient-descent)


# <span style="color:red">Book Chapter 4.</span>

### ğŸ“Œ What is Loss Function

#### 1. Definition of Loss Function

Loss Functionì€ ìš°ë¦¬ê°€ ë§Œë“  ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’ì´ ì‹¤ì œ ë°ì´í„°ì™€ ì°¨ì´ê°€ ì–¼ë§ˆë‚˜ ë‚˜ëŠ”ì§€ ìˆ˜ì¹˜ì ìœ¼ë¡œ ì •ëŸ‰í™”í•œ ì§€í‘œì´ë‹¤.

í•™ìŠµ, ì¦‰ fittingì„ ì§„í–‰í•  ë•Œ, ìš°ë¦¬ëŠ” input data xì™€ output data y ì‚¬ì´ì˜ ê´€ê³„ (mapping)ì„ ì°¾ì•„ê°€ê²Œ ëœë‹¤. ì´ ê³¼ì •ì—ì„œ ê°€ì¤‘ì¹˜ë¥¼ ì¡°ì •í•˜ê²Œ ë˜ë©°, loss functionì´ ì ì ˆí•œ ê°€ì¤‘ì¹˜ë¥¼ ì°¾ì•„ê°€ëŠ” ë°©í–¥ì„ ì œì‹œí•˜ê²Œ ë˜ëŠ” ê²ƒì´ë‹¤.

#### 2. Optimizer

lossë¥¼ ì¤„ì´ëŠ” ë°©í–¥ìœ¼ë¡œ í•™ìŠµì„ ì§„í–‰ì‹œí‚¤ëŠ” ì•Œê³ ë¦¬ì¦˜ì„ optimizerë¼ê³  í•œë‹¤. ê·¸ëŸ¼ ì–´ë–¤ optimizerê°€ ìˆì„ê¹Œ?


##### 2-1. ê¸°ë³¸í˜•: Gradient Descent (GD)

###### 2-1-1. Batch Gradient Descent

- ì „ì²´ ë°ì´í„°ì…‹ì„ ê¸°ì¤€ìœ¼ë¡œ í•œ ë²ˆì˜ ê²½ì‚¬ í•˜ê°•
- ì¥ì  : ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì„, ì—…ë°ì´íŠ¸ê°€ ë¹ ë¦„
- ì§„ë™ì´ ì‹¬í•˜ê³  ìµœì ì  ê·¼ì²˜ì—ì„œ ë¶ˆì•ˆì •í•¨.
  

<pre>'''matlab
function W = DeltaSGD(W, X, D)
    alpha =0.9;
    N =4;
    for k =1:N
        x = X(k, :);
        d = D(k);
        v = W*x;
        y = Sigmoid(v);
        e = d - y;
        delta = y*(1-y)*e;
        dW = alpha*delta*x;
        W(1) = W(1) + dW(1);
        W(2) = W(2) + dW(2);
        W(3) = W(3) + dW(3);
    end
end
'''
</pre>